# SQL & dbt ハンズオンレクチャー 説明資料

## 目次

1. [イントロダクション](#イントロダクション)
2. [第1コマ：SQLの基礎と応用](#第1コマsqlの基礎と応用)
3. [第2コマ：dbtでの実装](#第2コマdbtでの実装)
4. [まとめと次のステップ](#まとめと次のステップ)
5. [Q&A想定集](#qa想定集)

---

## イントロダクション

### ハンズオンの目的

本ハンズオンでは、データ変換未経験者向けに以下を学びます：

1. **SQLの基本から応用へ** - データ抽出から定期実行パイプラインまで
2. **dbtがもたらす改善** - テスト、ドキュメント、バージョン管理
3. **実務でのベストプラクティス** - Snowflakeでのデータ分析基盤構築

### テーマ：Webアクセスログ分析

実務でよくあるWebサイトのアクセスログを分析することで、実践的なスキルを習得します。

**対象者**
- データ変換未経験者
- SQL、dbtの知識がない初心者

**ボリューム**
- 2コマ（90分）
  - 第1コマ（45分）：SQL基礎→応用（ストアドプロシジャ、タスクまで）
  - 第2コマ（45分）：同じロジックをdbtで実装 → dbtの利点を体験

---

## 第1コマ：SQLの基礎と応用

### ステップ1：基本的なSELECT、WHERE、DISTINCT（5分）

**目的**：データの抽出と重複排除の基本

#### 実装例

```sql
-- 必要なカラムのみ取得
SELECT event_id, user_id, event_type, event_timestamp
FROM raw_events
LIMIT 10;

-- 特定条件でフィルタ
SELECT * FROM raw_events
WHERE event_type = 'purchase'
  AND country = 'US'
LIMIT 20;

-- ユニークな値を確認
SELECT DISTINCT country FROM raw_events ORDER BY country;

-- ユニークなユーザー数を計算
SELECT COUNT(DISTINCT user_id) AS unique_users FROM raw_events;
```

#### メリット/デメリット

| 項目 | 内容 |
|------|------|
| **メリット** | ・シンプルで理解しやすい ・データの初期確認に最適 |
| **デメリット** | ・複雑な加工は難しい ・手動で全ステップを記述する必要がある |

#### アンチパターン

❌ **避けるべき**：`SELECT *` の多用
```sql
SELECT * FROM raw_events;
```
→ 不要なカラムまで取得し、ネットワーク転送量が増加

✓ **推奨**：必要なカラムのみ指定
```sql
SELECT event_id, user_id, event_type FROM raw_events;
```

---

### ステップ2：JOIN（結合）（10分）

**目的**：複数テーブルの結合

#### INNER JOIN（両テーブルに存在するレコードのみ）

```sql
SELECT
    e.event_id,
    e.user_id,
    e.event_type,
    u.country,
    u.plan_type
FROM raw_events e
INNER JOIN users u ON e.user_id = u.user_id
LIMIT 20;
```

**特性**：
- user_idが raw_events と users 両方に存在するレコードのみ結果に含まれる
- ユーザー属性とイベント情報を統合

#### LEFT JOIN（左テーブルの全レコード）

```sql
SELECT
    e.event_id,
    e.user_id,
    u.signup_date,
    u.plan_type
FROM raw_events e
LEFT JOIN users u ON e.user_id = u.user_id;
```

**用途**：
- raw_eventsには存在するが usersには存在しない user_id を検出
- データ品質チェック（不正なuser_id検出）

#### 結合のメリット/デメリット

| JOIN種別 | メリット | デメリット |
|---------|---------|---------|
| **INNER** | 整合性のあるデータのみ | データ喪失の可能性 |
| **LEFT** | 全データ保持 | NULLの考慮が必要 |

#### アンチパターン

❌ **結合条件が不足**
```sql
-- 問題：user_id だけで結合し、session_id との関係を確認していない
SELECT * FROM raw_events e
JOIN sessions s ON e.user_id = s.user_id;
```

✓ **改善：複合キーで結合**
```sql
SELECT * FROM raw_events e
INNER JOIN sessions s
    ON e.session_id = s.session_id
    AND e.user_id = s.user_id;
```

---

### ステップ3：GROUP BY、集計関数（10分）

**目的**：データの集約と統計

#### 基本的な集計

```sql
SELECT
    DATE(event_timestamp) AS event_date,
    COUNT(*) AS total_events,
    COUNT(DISTINCT user_id) AS unique_users,
    COUNT(DISTINCT session_id) AS unique_sessions
FROM raw_events
GROUP BY DATE(event_timestamp)
ORDER BY event_date DESC;
```

**集計関数**：
- `COUNT(*)` ：行数（全て）
- `COUNT(DISTINCT user_id)` ：ユニーク値
- `SUM()` ：合計
- `AVG()` ：平均
- `MAX() / MIN()` ：最大値/最小値

#### 複数キーでの集計

```sql
SELECT
    DATE(e.event_timestamp) AS event_date,
    u.country,
    e.device_type,
    COUNT(*) AS event_count,
    COUNT(DISTINCT e.user_id) AS user_count,
    ROUND(COUNT(*)::FLOAT / COUNT(DISTINCT e.user_id), 2) AS avg_events_per_user
FROM raw_events e
INNER JOIN users u ON e.user_id = u.user_id
GROUP BY DATE(e.event_timestamp), u.country, e.device_type
ORDER BY event_date DESC, event_count DESC;
```

#### HAVING句でのフィルタリング

```sql
SELECT
    user_id,
    COUNT(*) AS event_count
FROM raw_events
GROUP BY user_id
HAVING COUNT(*) > 100  -- 集計後のフィルタ
ORDER BY event_count DESC;
```

**WHERE vs HAVING**：
- WHERE：グループ化前のフィルタ（個別行）
- HAVING：グループ化後のフィルタ（集計結果）

#### メリット/デメリット

| 項目 | 内容 |
|------|------|
| **メリット** | ・大量データを効率的に集約 ・KPI計算が容易 |
| **デメリット** | ・複雑な条件では可読性が低下 ・GROUP BY にないカラムを SELECT に含めるとエラー |

---

### ステップ4：CTE（WITH句）（5分）

**目的**：複雑なクエリの可読性向上

#### 基本的なCTE

```sql
WITH daily_events AS (
    SELECT
        DATE(event_timestamp) AS event_date,
        COUNT(*) AS event_count,
        COUNT(DISTINCT user_id) AS unique_users
    FROM raw_events
    GROUP BY DATE(event_timestamp)
)
SELECT *
FROM daily_events
WHERE event_date >= DATEADD(day, -30, CURRENT_DATE())
ORDER BY event_date DESC;
```

#### 複数CTEを使った段階的な構築

```sql
WITH daily_events AS (
    SELECT DATE(event_timestamp) AS event_date, COUNT(*) AS event_count
    FROM raw_events
    GROUP BY DATE(event_timestamp)
),

daily_purchases AS (
    SELECT DATE(event_timestamp) AS event_date, COUNT(*) AS purchase_count
    FROM raw_events
    WHERE event_type = 'purchase'
    GROUP BY DATE(event_timestamp)
)

SELECT
    e.event_date,
    e.event_count,
    COALESCE(p.purchase_count, 0) AS purchase_count,
    ROUND(COALESCE(p.purchase_count, 0)::FLOAT / e.event_count, 4) AS purchase_rate
FROM daily_events e
LEFT JOIN daily_purchases p ON e.event_date = p.event_date
ORDER BY e.event_date DESC;
```

#### メリット/デメリット

| 項目 | 内容 |
|------|------|
| **メリット** | ・可読性向上 ・デバッグが容易 ・サブクエリより理解しやすい |
| **デメリット** | ・若干のパフォーマンス overhead（Snowflakeでは最適化） |

---

### ステップ5：ビュー、マテリアライズドビュー（5分）

**目的**：よく使うクエリの再利用

#### VIEW（リアルタイムデータ参照）

```sql
CREATE OR REPLACE VIEW v_daily_events AS
SELECT
    DATE(event_timestamp) AS event_date,
    COUNT(*) AS event_count,
    COUNT(DISTINCT user_id) AS unique_users
FROM raw_events
GROUP BY DATE(event_timestamp);

-- 利用
SELECT * FROM v_daily_events
WHERE event_date >= CURRENT_DATE() - 30
ORDER BY event_date DESC;
```

**特性**：
- 仮想テーブル（実データは保持しない）
- クエリ実行時にベーステーブルをスキャン
- 常に最新データを参照

#### MATERIALIZED VIEW（事前計算済み）

```sql
CREATE OR REPLACE MATERIALIZED VIEW mv_daily_summary AS
SELECT
    DATE(event_timestamp) AS event_date,
    COUNT(*) AS event_count,
    COUNT(DISTINCT user_id) AS unique_users,
    COUNT(DISTINCT CASE WHEN event_type = 'purchase' THEN event_id END) AS purchase_count
FROM raw_events
GROUP BY DATE(event_timestamp);

-- 更新（手動または自動スケジュール）
ALTER MATERIALIZED VIEW mv_daily_summary REFRESH;
```

**特性**：
- 物理的なテーブルとして実データを保持
- 高速クエリが可能
- 手動更新が必要（リアルタイムではない）

#### VIEW vs MATERIALIZED VIEW

| 項目 | VIEW | MATERIALIZED VIEW |
|------|------|------------------|
| **ストレージ** | 不要 | 必要 |
| **データ鮮度** | リアルタイム | 更新時点 |
| **パフォーマンス** | 低い（毎回スキャン） | 高い（事前計算） |
| **用途** | リアルタイムデータ必須時 | 複雑集計をよく参照時 |

---

### ステップ6：ストアドプロシジャ（5分）

**目的**：複数ステップの処理をまとめる

#### 基本的なストアドプロシジャ

```sql
CREATE OR REPLACE PROCEDURE sp_calculate_daily_summary()
RETURNS VARCHAR
LANGUAGE SQL
AS
$$
BEGIN
    DELETE FROM daily_summary;

    INSERT INTO daily_summary (event_date, event_count, unique_users)
    SELECT
        DATE(event_timestamp) AS event_date,
        COUNT(*) AS event_count,
        COUNT(DISTINCT user_id) AS unique_users
    FROM raw_events
    GROUP BY DATE(event_timestamp);

    RETURN '✓ Processing completed';
END;
$$;

-- 実行
CALL sp_calculate_daily_summary();
```

#### パラメータ付きプロシジャ

```sql
CREATE OR REPLACE PROCEDURE sp_calculate_by_country(p_country VARCHAR)
RETURNS VARCHAR
LANGUAGE SQL
AS
$$
BEGIN
    INSERT INTO daily_summary_by_country (event_date, country, event_count)
    SELECT
        DATE(e.event_timestamp) AS event_date,
        p_country AS country,
        COUNT(*) AS event_count
    FROM raw_events e
    INNER JOIN users u ON e.user_id = u.user_id
    WHERE u.country = p_country
    GROUP BY DATE(e.event_timestamp);

    RETURN 'Processing completed for: ' || p_country;
END;
$$;

-- 実行
CALL sp_calculate_by_country('US');
CALL sp_calculate_by_country('JP');
```

#### メリット/デメリット

| 項目 | 内容 |
|------|------|
| **メリット** | ・複雑な処理を1つにまとめられる ・データベース側で実行（効率的） |
| **デメリット** | ・テストが難しい ・デバッグが困難 ・バージョン管理が複雑 ・可読性が低い |

#### アンチパターン

❌ **避けるべき**：過度に複雑なプロシジャ
```sql
-- 数百行のプロシジャで複数の責務を負う
CREATE PROCEDURE complex_pipeline() AS ...
```

✓ **推奨**：処理を分割
```sql
CREATE PROCEDURE step1_daily_summary() AS ...
CREATE PROCEDURE step2_weekly_summary() AS ...
CREATE PROCEDURE step3_active_users() AS ...
```

**重要**：ステップ8（dbt）では、このプロシジャの課題を解決します。

---

### ステップ7：タスク（5分）

**目的**：定期実行パイプラインの構築

#### 基本的なタスク

```sql
CREATE OR REPLACE TASK tsk_daily_summary
WAREHOUSE = task_wh
SCHEDULE = 'USING CRON 0 1 * * * UTC'  -- 毎日 01:00 UTC
AS
CALL sp_calculate_daily_summary();

ALTER TASK tsk_daily_summary RESUME;
```

#### タスク間の依存関係

```sql
-- 親タスク
CREATE OR REPLACE TASK tsk_parent_daily
WAREHOUSE = task_wh
SCHEDULE = 'USING CRON 0 1 * * * UTC'
AS
CALL sp_calculate_daily_summary();

-- 子タスク（親完了後に実行）
CREATE OR REPLACE TASK tsk_child_weekly
WAREHOUSE = task_wh
AFTER tsk_parent_daily
AS
INSERT INTO weekly_summary (...)
SELECT ...
FROM daily_summary
GROUP BY ...;

ALTER TASK tsk_parent_daily RESUME;
ALTER TASK tsk_child_weekly RESUME;
```

#### スケジュール例

```
SCHEDULE = 'USING CRON 0 1 * * * UTC'         毎日 01:00
SCHEDULE = 'USING CRON 0 */6 * * * UTC'       6時間ごと
SCHEDULE = 'USING CRON 0 1 * * 1 UTC'         毎週月曜
SCHEDULE = 'USING CRON 0 1 1 * * UTC'         毎月1日
```

#### メリット/デメリット

| 項目 | 内容 |
|------|------|
| **メリット** | ・自動化により手作業削減 ・定期実行が容易 ・スケーラビリティ |
| **デメリット** | ・エラー監視が必要 ・トラブルシューティングが複雑 |

---

## 第2コマ：dbtでの実装

### ステップ8：dbt on Snowflake環境のセットアップ（5分）

**目的**：Snowflake内でdbtプロジェクトを管理・実行する環境を構築

#### dbt on Snowflake とは

**通常のdbt CLI**
```
ローカル → dbtインストール → dbt run → Snowflake
```

**dbt on Snowflake**
```
Snowflake内でプロジェクト管理・実行
Git統合、UI操作可能
```

#### セットアップ手順

1. **Snowflakeでのテーブル作成**
```sql
CREATE DATABASE analytics;
CREATE SCHEMA analytics.staging;
CREATE SCHEMA analytics.intermediate;
CREATE SCHEMA analytics.marts;
```

2. **dbtプロジェクト構造**
```
dbt_project/
├── dbt_project.yml
├── models/
│   ├── staging/
│   │   ├── stg_events.sql
│   │   └── stg_users.sql
│   ├── intermediate/
│   │   └── int_daily_events.sql
│   └── marts/
│       ├── daily_summary.sql
│       └── weekly_summary.sql
├── macros/
├── tests/
└── snapshots/
```

3. **Git統合**
- GitHub/GitLabと連携
- コマンドの自動実行可能

#### dbt on Snowflakeの利点

✓ **Git統合**：コード完全にバージョン管理
✓ **自動テスト**：データ品質チェック自動化
✓ **ドキュメント自動生成**：Lineage可視化
✓ **UI操作**：ブラウザからの実行・監視

---

### ステップ9：dbtモデル（基本）（10分）

**目的**：SQLのCTEをdbtモデルに置き換える

#### Staging層（stg_events）

```sql
{{ config(
    materialized='view',
    tags=['staging'],
    description='前処理済みイベントログ'
) }}

WITH source_events AS (
    SELECT * FROM {{ source('analytics', 'raw_events') }}
),

cleaned_events AS (
    SELECT
        event_id,
        user_id,
        UPPER(event_type) AS event_type,
        event_timestamp,
        DATE(event_timestamp) AS event_date,
        CASE
            WHEN event_type = 'PAGE_VIEW' THEN 'Engagement'
            WHEN event_type IN ('CHECKOUT', 'PURCHASE') THEN 'Conversion'
            ELSE 'Other'
        END AS funnel_stage
    FROM source_events
    WHERE event_id IS NOT NULL
)

SELECT * FROM cleaned_events
```

**特徴**：
- `{{ config(...) }}` で構成（マテリアライズ、説明等）
- `{{ source(...) }}` でソーステーブル参照（依存関係を dbt が認識）
- `{{ ref(...) }}` で他のモデル参照

#### Intermediate層（int_daily_events）

```sql
{{ config(
    materialized='view',
    tags=['intermediate']
) }}

WITH events AS (
    SELECT * FROM {{ ref('stg_events') }}
),

users AS (
    SELECT * FROM {{ ref('stg_users') }}
),

joined AS (
    SELECT
        e.event_date,
        e.funnel_stage,
        u.country,
        u.plan_type,
        COUNT(*) AS event_count,
        COUNT(DISTINCT e.user_id) AS user_count
    FROM events e
    INNER JOIN users u ON e.user_id = u.user_id
    GROUP BY e.event_date, e.funnel_stage, u.country, u.plan_type
)

SELECT * FROM joined
```

**ref() の利点**：
- 依存関係を dbt が自動認識 → DAG生成
- モデル名変更時も自動で追跡
- 実行順序を dbt が自動決定

#### dbtモデルのメリット

✓ **ファイル分割で可読性向上**
✓ **依存関係の自動管理（DAG）**
✓ **1コマンド（dbt run）で一括実行**
✓ **Git連携でバージョン管理が容易**
✓ **SQL + dbt構文でテスト容易**

#### 層の役割分け

| 層 | Materialization | 役割 | 例 |
|----|----------------|-----|-----|
| **staging** | VIEW | ソースデータの前処理 | stg_events, stg_users |
| **intermediate** | VIEW | 複数テーブル結合、加工 | int_daily_events |
| **marts** | TABLE | ビジネスユーザー向け最終テーブル | daily_summary |

---

### ステップ10：dbtモデル（Materialization）（10分）

**目的**：ビュー、テーブル、incremental modelの使い分け

#### View（軽量、最新データ）

```sql
{{ config(materialized='view') }}

SELECT ... FROM ...
```

**特性**：
- ストレージ不要
- 常に最新データ参照
- パフォーマンスは低い（毎回フルスキャン）

**用途**：
- 小～中規模データセット
- リアルタイムデータ必須時
- staging層、intermediate層

#### Table（パフォーマンス重視）

```sql
{{ config(materialized='table') }}

SELECT ... FROM ...
```

**特性**：
- 物理テーブルとして保存
- 高速クエリが可能
- ストレージを消費

**用途**：
- 複雑集計の結果
- よくアクセスされるデータ
- marts層

#### Incremental（差分更新）

```sql
{{ config(
    materialized='incremental',
    unique_key='event_id'
) }}

SELECT * FROM raw_events

{% if execute %}
    WHERE event_timestamp > (
        SELECT MAX(event_timestamp) FROM {{ this }}
    )
{% endif %}
```

**特性**：
- 初回：全データ読込み
- 2回目以降：新しいデータのみ追加
- 大規模データに有効

**用途**：
- 増分データの効率的な処理
- イベントログの日次更新

#### Materialization比較表

| 項目 | View | Table | Incremental |
|------|------|-------|-------------|
| **ストレージ** | 不要 | 必要 | 必要 |
| **パフォーマンス** | 低 | 高 | 高 |
| **データ鮮度** | リアルタイム | 更新時点 | 更新時点 |
| **セットアップ** | 簡単 | 簡単 | 中程度 |
| **用途** | 小規模 | 中～大規模 | 大規模増分 |

---

### ステップ11：dbtテスト（5分）

**目的**：データ品質の自動テスト

#### テスト定義（schema.yml）

```yaml
models:
  - name: stg_events
    columns:
      - name: event_id
        tests:
          - unique
          - not_null
      - name: event_type
        tests:
          - accepted_values:
              values: ['page_view', 'click', 'purchase']

  - name: daily_summary
    columns:
      - name: purchase_rate
        tests:
          - dbt_utils.accepted_range:
              min_value: 0
              max_value: 1
```

#### テスト種別

```sql
-- 1. Unique（一意性）
tests:
  - unique

-- 2. Not Null（NULL非許容）
tests:
  - not_null

-- 3. Accepted Values（許容値）
tests:
  - accepted_values:
      values: ['value1', 'value2']

-- 4. Relationships（外部キー制約）
tests:
  - relationships:
      to: ref('target_model')
      field: target_column
```

#### テスト実行

```bash
dbt test                    # すべてのテスト実行
dbt test -s stg_events      # 特定モデルのみ
dbt test --fail-fast        # 最初のエラーで停止
```

#### メリット

✓ **自動データ品質チェック**
✓ **デグレレーション検出**
✓ **本番環境への自信**
✓ **ドキュメントとしても機能**

---

### ステップ12：dbtドキュメント（5分）

**目的**：自動ドキュメント生成

#### ドキュメント定義

```yaml
models:
  - name: daily_summary
    description: "日別パフォーマンスサマリー"
    columns:
      - name: event_date
        description: "イベント発生日"
      - name: country
        description: "国コード"
      - name: purchase_rate
        description: "購入レート（購入数 / ユーザー数）"
```

#### ドキュメント生成

```bash
dbt docs generate
dbt docs serve
```

**生成内容**：
- モデルのスキーマ情報
- テーブル・カラムの説明
- **Lineage DAG**（データの系譜）
- テスト定義

#### Lineageの価値

```
raw_events → stg_events → int_daily_events → daily_summary
    ↓           ↓              ↓                   ↓
 users      stg_users    [JOIN][集計]        [最終テーブル]
```

**利点**：
✓ **データの流れを可視化**
✓ **影響分析（どのモデルを変更したら何が変わるか）**
✓ **新人研修が容易**

---

### ステップ13：ストアドプロシジャ→dbt置き換え（10分）

**目的**：ストアドプロシジャ + タスク をdbtで置き換え

#### SQL（ステップ6）の課題

```sql
-- ストアドプロシジャの問題
CREATE PROCEDURE sp_run_nightly_pipeline() AS
BEGIN
    -- ステップ1：日別集計
    DELETE FROM daily_summary;
    INSERT INTO daily_summary ...;

    -- ステップ2：週別集計
    DELETE FROM weekly_summary;
    INSERT INTO weekly_summary ...;

    -- ステップ3：アクティブユーザー
    DELETE FROM active_users;
    INSERT INTO active_users ...;
END;
```

**問題**：
❌ 全ロジックが1つのプロシジャ → デバッグ困難
❌ テストが書きにくい → 品質低下
❌ バージョン管理が複雑 → Git追跡困難

#### dbtでの置き換え

```sql
-- models/marts/daily_summary.sql
{{ config(materialized='table') }}

WITH daily_events AS (
    SELECT * FROM {{ ref('int_daily_events') }}
)
SELECT
    event_date,
    country,
    COUNT(*) AS event_count,
    ...
FROM daily_events
GROUP BY event_date, country;

-- models/marts/weekly_summary.sql
{{ config(materialized='table') }}

WITH daily_summary AS (
    SELECT * FROM {{ ref('daily_summary') }}
)
SELECT
    DATE_TRUNC('WEEK', event_date) AS week_start,
    SUM(event_count) AS weekly_events,
    ...
FROM daily_summary
GROUP BY DATE_TRUNC('WEEK', event_date);
```

#### Pre/Post Hooks（処理の前後に実行）

```sql
{{ config(
    pre_hook="DELETE FROM staging_table;",
    post_hook="GRANT SELECT ON TABLE {{ this }} TO ROLE analyst;"
) }}

SELECT * FROM source_table;
```

#### Macros（共通ロジック再利用）

```sql
-- macros/common_logic.sql
{% macro funnel_stage_generator(event_type_col) %}
    CASE
        WHEN {{ event_type_col }} = 'PAGE_VIEW' THEN 'Engagement'
        WHEN {{ event_type_col }} IN ('CHECKOUT', 'PURCHASE') THEN 'Conversion'
    END
{% endmacro %}

-- models/staging/stg_events.sql
SELECT
    event_id,
    user_id,
    {{ funnel_stage_generator('event_type') }} AS funnel_stage
FROM {{ source('analytics', 'raw_events') }};
```

#### Snowflakeタスクとdbtの統合

```sql
-- Snowflake Task で dbt を定期実行
CREATE OR REPLACE TASK dbt_daily_run
WAREHOUSE = dbt_wh
SCHEDULE = 'USING CRON 0 1 * * * UTC'
AS
EXECUTE DBT PROJECT analytics_web_events COMMAND = 'dbt run';

ALTER TASK dbt_daily_run RESUME;

-- テストも定期実行
CREATE OR REPLACE TASK dbt_daily_test
WAREHOUSE = dbt_wh
AFTER dbt_daily_run
AS
EXECUTE DBT PROJECT analytics_web_events COMMAND = 'dbt test';

ALTER TASK dbt_daily_test RESUME;
```

#### dbtへの移行メリット

| 項目 | SQL | dbt |
|------|-----|-----|
| **テスト性** | ❌ 困難 | ✓ 容易 |
| **ドキュメント** | ❌ 手動 | ✓ 自動生成 |
| **Git連携** | ❌ 弱い | ✓ 完全統合 |
| **保守性** | ❌ 低い | ✓ 高い |
| **再利用性** | ❌ 低い | ✓ 高い（macros） |
| **デバッグ** | ❌ 困難 | ✓ 容易 |

---

### SQL vs dbt：同じロジックの比較

第1コマのSQLと第2コマのdbtは**同じビジネスロジック**を実装しています。
以下に対応関係を示します。

#### 日別集計ロジックの比較

**SQL版（ステップ3〜4で学習）**
```sql
-- 日別のイベント集計（CTEで段階的に構築）
WITH daily_events AS (
    SELECT
        DATE(e.event_timestamp) AS event_date,
        u.country,
        u.plan_type,
        COUNT(*) AS event_count,
        COUNT(DISTINCT e.user_id) AS unique_users,
        COUNT(DISTINCT CASE WHEN e.event_type = 'purchase' THEN e.event_id END) AS purchase_count
    FROM raw_events e
    INNER JOIN users u ON e.user_id = u.user_id
    GROUP BY DATE(e.event_timestamp), u.country, u.plan_type
)
SELECT
    event_date,
    country,
    plan_type,
    event_count,
    unique_users,
    ROUND(purchase_count::FLOAT / unique_users, 4) AS purchase_rate
FROM daily_events
ORDER BY event_date DESC;
```

**dbt版（ステップ9〜10で学習）**
```sql
-- models/intermediate/int_daily_events.sql
{{ config(materialized='view') }}

WITH events AS (
    SELECT * FROM {{ ref('stg_events') }}    -- ← source() ではなく ref() で依存管理
),
users AS (
    SELECT * FROM {{ ref('stg_users') }}     -- ← 前処理済みデータを参照
),
...
SELECT ... FROM events INNER JOIN users ...

-- models/marts/daily_summary.sql
{{ config(materialized='table') }}           -- ← materialization で保存方法を制御

WITH daily_events AS (
    SELECT * FROM {{ ref('int_daily_events') }}  -- ← 中間モデルを参照
)
SELECT ..., ROUND(purchase_events::FLOAT / NULLIF(unique_users, 0), 4) AS purchase_rate
FROM daily_events
```

**違いのポイント**

| 観点 | SQL版 | dbt版 |
|------|-------|-------|
| **テーブル参照** | `FROM raw_events e` | `FROM {{ ref('stg_events') }}` |
| **保存方法** | 手動で `CREATE TABLE` / `INSERT` | `{{ config(materialized='table') }}` |
| **前処理** | 同じクエリ内で実施 | staging モデルに分離 |
| **テスト** | 手動で `SELECT` 検証 | `schema.yml` で自動テスト |
| **依存関係** | 実行順序を手動管理 | `ref()` で自動管理 |

---

### 演習課題（手を動かして学ぶ）

以下の演習に挑戦して、dbtの実践的な使い方を体験しましょう。

#### 演習1：stg_events に新しいカラムを追加

**課題**：`stg_events.sql` に `is_purchase`（BOOLEAN型）カラムを追加してください。

```sql
-- models/staging/stg_events.sql の cleaned_events CTE に以下を追加：
        (UPPER(event_type) = 'PURCHASE') AS is_purchase
```

追加後、実行して確認：
```bash
dbt run -s stg_events
```

#### 演習2：テスト定義の追加

**課題**：`tests/schema.yml` の `stg_events` モデルに `is_purchase` の `not_null` テストを追加してください。

```yaml
# tests/schema.yml の stg_events セクションに追加：
      - name: is_purchase
        description: "購入イベントかどうか（TRUE/FALSE）"
        tests:
          - not_null
```

追加後、テストを実行：
```bash
dbt test -s stg_events
```

#### 演習3：マクロの効果を体験

**課題**：`stg_events.sql` のファネルステージ生成がマクロ `funnel_stage_generator` を使っていることを確認し、マクロの定義（`macros/common_logic.sql`）を変更してみましょう。

例：`SIGN_UP` のステージを `'Acquisition'` から `'Registration'` に変更
→ `dbt run` すると、マクロを参照する全モデルに変更が反映されることを確認

---

## まとめと次のステップ

### SQLとdbtの使い分け

#### SQLを使う場面

- データの初期確認（SELECT, WHERE, DISTINCT）
- アドホック分析
- シンプルな処理

#### dbtを使う場面（推奨）

- 定期的に実行するパイプライン
- 複数モデルの組み合わせ
- テスト・品質管理が重要
- チーム開発
- バージョン管理が必須

### 実務での推奨アーキテクチャ

```
Raw Data (S3/API)
    ↓
[dbt Staging] → Snowflake RAW_EVENTS
    ↓
[dbt Intermediate] → Snowflake INTERMEDIATE
    ↓
[dbt Marts] → Snowflake MARTS
    ↓
BI ツール（Tableau, Looker等）
    ↓
ビジネスダッシュボード
```

### 次のステップ（学習リソース）

1. **dbt公式ドキュメント**
   - https://docs.getdbt.com

2. **dbt Learn**
   - 無料オンラインコース
   - 実践的なプロジェクト例

3. **dbt Slack コミュニティ**
   - 質問・相談の場

4. **Snowflake + dbt連携**
   - Snowflakeブログ
   - Snowflake dbt Integration ガイド

5. **高度なトピック**
   - Dynamic Tables（Snowflakeの自動更新機能）
   - dbt Metrics（KPI定義の標準化）
   - dbt Cloud（マネージドサービス）

---

## Q&A想定集

### Q1：dbtは必須ですか？

**A**：小規模プロジェクト（<100万行/日）であれば、Snowflake Taskで十分です。
ただし以下の場合はdbt推奨：
- チーム開発
- テスト・品質管理が重要
- 複雑な変換ロジック
- 将来の拡張を見据えている

### Q2：既存のストアドプロシジャはどうする？

**A**：段階的に移行してください：
1. 新規パイプライン → dbtで実装
2. 既存パイプライン → 優先度高いものから dbt に移行
3. 古いプロシジャ → 廃止

### Q3：パフォーマンスはどちらが良い？

**A**：同等です。以下に注意：
- 不要なカラムは SELECT しない（SQL, dbt両方）
- JOINの順序を最適化（dbtも Snowflake が自動最適化）
- インデックス活用（Snowflakeでは CLUSTERING キー）

### Q4：Incremental modelはいつ使う？

**A**：以下の条件下：
- **大規模データ**（100万件/日以上）
- **増分処理が可能**（タイムスタンプで差分判定可能）
- **パフォーマンスが重要**（毎日の実行時間が問題）

### Q5：テストはどのくらい詳細に書く？

**A**：最初は基本テスト：
- 主キー： `unique`
- 必須フィールド： `not_null`
- 分類値： `accepted_values`

詳細テストは後から追加：
- データ範囲チェック（0-100%等）
- 外部キー検証
- 業務ルール検証

### Q6：dbtクラウドは必須？

**A**：いいえ。以下で選択：
- **dbt Cloud（有料）** → UI操作、スケジューリング機能充実
- **dbt on Snowflake（無料）** → Git統合、Snowflake UI で十分
- **ローカル dbt CLI（無料）** → 開発時のみ

### Q7：Git統合しない場合は？

**A**：可能ですが非推奨です：
- バージョン管理が困難
- 誰が何を変更したか追跡不可
- ロールバック時の手作業が煩雑

最低限 dbt_project.yml 等をGitで管理してください。

### Q8：本番環境での実行頻度は？

**A**：ビジネス要件によりますが、一般的には：
- **リアルタイム関連**（ユーザー行動） → 1時間ごと
- **日次レポート** → 毎日1時間ごと（深夜）
- **週次分析** → 毎週月曜朝
- **月次決算** → 月初

### Q9：失敗時の対応は？

**A**：以下を設定：
```sql
CREATE ALERT task_failed_alert
IF (EXISTS(
    SELECT 1 FROM TASK_HISTORY
    WHERE STATE = 'FAILED'
))
THEN
    -- メール通知等
END;
```

### Q10：他のDBMS（PostgreSQL, BigQuery等）でも使える？

**A**：はい。dbtは複数DBMS対応：
- Snowflake（推奨）
- PostgreSQL
- BigQuery
- Redshift
- Databricks
- 他多数

ただしSnowflake固有の機能（Dynamic Tables, Clustering等）は未対応。

---

## 補足：実装のコツ

### SQLベストプラクティス

1. **必要なカラムのみ SELECT**
   ```sql
   ✓ SELECT event_id, user_id, event_type FROM ...
   ❌ SELECT * FROM ...
   ```

2. **フィルタリングは早期に**
   ```sql
   ✓ WHERE event_timestamp >= DATE_TRUNC('WEEK', CURRENT_DATE())
   ❌ WHERE WEEK(event_timestamp) >= ...  (関数で計算)
   ```

3. **複雑なロジックは CTE で分割**
   ```sql
   WITH layer1 AS (SELECT ...),
   layer2 AS (SELECT ... FROM layer1),
   layer3 AS (SELECT ... FROM layer2)
   SELECT * FROM layer3;
   ```

4. **GROUP BY にない列を SELECT しない**
   ```sql
   ✓ SELECT country, COUNT(*) FROM ... GROUP BY country
   ❌ SELECT country, user_id, COUNT(*) FROM ... GROUP BY country
   ```

### dbtベストプラクティス

1. **モデル名は層を示す**
   - `stg_` ：staging 層
   - `int_` ：intermediate 層
   - `fct_` ：fact テーブル（事実）
   - `dim_` ：dimension テーブル（属性）

2. **テストは必須**
   ```yaml
   - unique
   - not_null
   - accepted_values
   ```

3. **ドキュメント必須**
   ```yaml
   description: "..."
   columns:
     - name: column_name
       description: "説明"
   ```

4. **Macros で再利用**
   - 複数モデルで同じロジック
   - 複雑な計算式
   - 共通フォーマット

---

**このハンズオンの終了後、実務での応用に向けて以下を継続学習してください：**

- dbt Advancedトピック（Jinja2テンプレート、カスタムテスト等）
- Snowflake最適化（CLUSTERING, PARTITIONING, キャッシング）
- パイプラインの監視・ロギング
- BI ツールとの連携

**質問・フィードバックは随時受け付けています。**
